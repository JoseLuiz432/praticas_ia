{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático Modulo Final\n",
    "\n",
    "## Enunciado\n",
    "\n",
    "O projeto visa explorar o potencial dos Conditional Variational Autoencoders (C-VAEs) para a geração de dados sintéticos baseados em classes específicas de imagens do dataset MNIST. O MNIST é um conjunto de dados composto por milhares de imagens de dígitos escritos à mão.\n",
    "\n",
    "Para este trabalho prático, você desenvolverá uma C-VAEs para gerar novas imagens que se assemelhem aos dados originais. A C-VAEs será treinada para aprender as representações latentes dos dados e gerar amostras que imitam as características principais das imagens de entrada.\n",
    "\n",
    "Será disponibilizado um Jupyter notebook (.ipynb) com o nome “trabalho_pratico_guia.ipynb” para auxiliar no desenvolvimento da C-VAE.\n",
    "\n",
    "## Passo 1: Editar o script training_script.py disponibilizado\n",
    "1. Construa o encoder (lembre que a entrada é uma imagem 28x28 Mnist)\n",
    "2. Construa o decoder (a saída é também uma imagem 28x28)\n",
    "3. Modifique predict_fn de modo que você realize uma predição sobre os dados de entrada.\n",
    "\n",
    "Obs.: Os métodos predict_fn, input_fn, model_fn são utilizado pelo SageMaker endpoint para realizar a predição, tratar o input e carregar o modelo respectivamente.\n",
    "\n",
    "## Criando um ambiente no SageMaker\n",
    "\n",
    "1. Acesse o console da AWS e faça login como root\n",
    "    > Caso não tenha uma conta na AWS crie uma nova conta.\n",
    "    > Consulte o nível gratuito do SM [AQUI](https://aws.amazon.com/pt/pm/sagemaker/?gclid=CjwKCAjwl6-3BhBWEiwApN6_knt7Co9rU8YwhSbR4ePsaeYuzgex5QwFdWibp1jL53CM3YXZ5IT4EBoCoUkQAvD_BwE&trk=41368dcc-5040-4349-998b-a9c524544f65&sc_channel=ps&ef_id=CjwKCAjwl6-3BhBWEiwApN6_knt7Co9rU8YwhSbR4ePsaeYuzgex5QwFdWibp1jL53CM3YXZ5IT4EBoCoUkQAvD_BwE:G:s&s_kwcid=AL!4422!3!532488969034!e!!g!!sagemaker!12024811143!113992794377)\n",
    "2. Nos serviços da AWS busque por SageMaker Studio e entre.\n",
    "3. No Applications and IDEs -> Studio clique em *Create a SageMaker Domain*\n",
    "4. Abra o studio criado e vá para JupyterLab e crie um novo espaço do JupyterLab\n",
    "5. Depois de criado o espaço entre nele e na lateral direita superior clique em upload files e suba esse arquivo .ipynb e o training_script.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas necessárias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup do bucket\n",
    "\n",
    "Se você quiser usar um bucket S3 específico, use o código a seguir e substitua as strings pelo nome exato do bucket S3. O nome do bucket deve conter sagemaker e ser globalmente exclusivo. O bucket deve estar na mesma região da AWS que a instância de notebook usada neste exemplo.\n",
    "\n",
    "Caso não queira criar seu próprio bucket para esse trabalho, o SageMaker vai criar um para você. Para isso remova o default_bucket do sagemaker.Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"sagemaker-testets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicie a sessão do SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session(default_bucket = bucket)\n",
    "\n",
    "# Obtém o papel de execução do SageMaker (isso pode não funcionar fora do ambiente SageMaker)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/demo_mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo os dados MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Carregando o dataset MNIST\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subindo os dados para um S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-testets/sagemaker/demo_mnist\n"
     ]
    }
   ],
   "source": [
    "# Sobe os dados da pasta \"data\" para o s3 bucket default\n",
    "inputs = sagemaker_session.upload_data(path=\"data\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando alguns dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "# Visualizando algumas imagens do dataset MNIST\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig, axs = plt.subplots(1, 6, figsize=(15, 3))\n",
    "for i in range(6):\n",
    "    axs[i].imshow(example_data[i][0], cmap='gray')\n",
    "    axs[i].set_title(f'Label: {example_targets[i].item()}')\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subindo o primeiro estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtqdm\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# from dotenv import load_dotenv\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "logger.setLevel(logging.DEBUG)\u001b[37m\u001b[39;49;00m\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mConditionalEncoder\u001b[39;49;00m(nn.Module):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_channels, num_classes, latent_dim):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36msuper\u001b[39;49;00m(ConditionalEncoder, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(input_channels + num_classes, \u001b[34m32\u001b[39;49;00m, kernel_size=\u001b[34m4\u001b[39;49;00m, stride=\u001b[34m2\u001b[39;49;00m, padding=\u001b[34m1\u001b[39;49;00m)  \u001b[37m# [batch_size, 32, 14, 14]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m32\u001b[39;49;00m, \u001b[34m64\u001b[39;49;00m, kernel_size=\u001b[34m4\u001b[39;49;00m, stride=\u001b[34m2\u001b[39;49;00m, padding=\u001b[34m1\u001b[39;49;00m)  \u001b[37m# [batch_size, 64, 7, 7]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.conv3 = nn.Conv2d(\u001b[34m64\u001b[39;49;00m, \u001b[34m128\u001b[39;49;00m, kernel_size=\u001b[34m4\u001b[39;49;00m, stride=\u001b[34m2\u001b[39;49;00m, padding=\u001b[34m1\u001b[39;49;00m)  \u001b[37m# [batch_size, 128, 3, 3]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.flatten = nn.Flatten()  \u001b[37m# Flatten para [batch_size, 128*3*3]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.fc_mu = nn.Linear(\u001b[34m128\u001b[39;49;00m * \u001b[34m3\u001b[39;49;00m * \u001b[34m3\u001b[39;49;00m, latent_dim)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.fc_logvar = nn.Linear(\u001b[34m128\u001b[39;49;00m * \u001b[34m3\u001b[39;49;00m * \u001b[34m3\u001b[39;49;00m, latent_dim)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x, labels):\u001b[37m\u001b[39;49;00m\n",
      "        labels = labels.view(labels.size(\u001b[34m0\u001b[39;49;00m), labels.size(\u001b[34m1\u001b[39;49;00m), \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        labels = labels.expand(labels.size(\u001b[34m0\u001b[39;49;00m), labels.size(\u001b[34m1\u001b[39;49;00m), x.size(\u001b[34m2\u001b[39;49;00m), x.size(\u001b[34m3\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        x = torch.cat((x, labels), dim=\u001b[34m1\u001b[39;49;00m) \u001b[37m# concatenando as labels\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        h = torch.relu(\u001b[36mself\u001b[39;49;00m.conv1(x))\u001b[37m\u001b[39;49;00m\n",
      "        h = torch.relu(\u001b[36mself\u001b[39;49;00m.conv2(h))\u001b[37m\u001b[39;49;00m\n",
      "        h = torch.relu(\u001b[36mself\u001b[39;49;00m.conv3(h))\u001b[37m\u001b[39;49;00m\n",
      "        h = \u001b[36mself\u001b[39;49;00m.flatten(h)\u001b[37m\u001b[39;49;00m\n",
      "        mu = \u001b[36mself\u001b[39;49;00m.fc_mu(h)\u001b[37m\u001b[39;49;00m\n",
      "        logvar = \u001b[36mself\u001b[39;49;00m.fc_logvar(h)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m mu, logvar\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mConditionalDecoder\u001b[39;49;00m(nn.Module):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, latent_dim, num_classes, output_channels):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36msuper\u001b[39;49;00m(ConditionalDecoder, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.fc = nn.Linear(latent_dim + num_classes, \u001b[34m128\u001b[39;49;00m * \u001b[34m3\u001b[39;49;00m * \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.deconv1 = nn.ConvTranspose2d(\u001b[34m128\u001b[39;49;00m, \u001b[34m64\u001b[39;49;00m, kernel_size=\u001b[34m4\u001b[39;49;00m, stride=\u001b[34m2\u001b[39;49;00m, padding=\u001b[34m1\u001b[39;49;00m)  \u001b[37m# [batch_size, 64, 6, 6]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.deconv2 = nn.ConvTranspose2d(\u001b[34m64\u001b[39;49;00m, \u001b[34m32\u001b[39;49;00m, kernel_size=\u001b[34m4\u001b[39;49;00m, stride=\u001b[34m2\u001b[39;49;00m, padding=\u001b[34m0\u001b[39;49;00m)  \u001b[37m# [batch_size, 32, 14, 14]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.deconv3 = nn.ConvTranspose2d(\u001b[34m32\u001b[39;49;00m, output_channels, kernel_size=\u001b[34m4\u001b[39;49;00m, stride=\u001b[34m2\u001b[39;49;00m, padding=\u001b[34m1\u001b[39;49;00m)  \u001b[37m# [batch_size, 1, 28, 28]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, z, labels):\u001b[37m\u001b[39;49;00m\n",
      "        z = torch.cat((z, labels), dim=\u001b[34m1\u001b[39;49;00m) \u001b[37m# concatenando as labels\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        h = \u001b[36mself\u001b[39;49;00m.fc(z)\u001b[37m\u001b[39;49;00m\n",
      "        h = h.view(-\u001b[34m1\u001b[39;49;00m, \u001b[34m128\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)  \u001b[37m# Redimensionar para [batch_size, 128, 3, 3]\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        h = torch.relu(\u001b[36mself\u001b[39;49;00m.deconv1(h))\u001b[37m\u001b[39;49;00m\n",
      "        h = torch.relu(\u001b[36mself\u001b[39;49;00m.deconv2(h))\u001b[37m\u001b[39;49;00m\n",
      "        x_reconstructed = torch.sigmoid(\u001b[36mself\u001b[39;49;00m.deconv3(h))\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m x_reconstructed\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mCVAE\u001b[39;49;00m(nn.Module):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_channels, num_classes, latent_dim):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36msuper\u001b[39;49;00m(CVAE, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.encoder = ConditionalEncoder(input_channels, num_classes, latent_dim)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.decoder = ConditionalDecoder(latent_dim, num_classes, input_channels)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mreparameterize\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, mu, logvar):\u001b[37m\u001b[39;49;00m\n",
      "        std = torch.exp(logvar)\u001b[37m\u001b[39;49;00m\n",
      "        eps = torch.randn_like(std)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m mu + eps * std\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x, labels):\u001b[37m\u001b[39;49;00m\n",
      "        mu, logvar = \u001b[36mself\u001b[39;49;00m.encoder(x, labels)\u001b[37m\u001b[39;49;00m\n",
      "        z = \u001b[36mself\u001b[39;49;00m.reparameterize(mu, logvar)\u001b[37m\u001b[39;49;00m\n",
      "        x_reconstructed = \u001b[36mself\u001b[39;49;00m.decoder(z, labels)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m x_reconstructed, mu, logvar\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mloss_function\u001b[39;49;00m(x_reconstructed, x, mu, logvar):\u001b[37m\u001b[39;49;00m\n",
      "    reconstruction_loss = nn.functional.binary_cross_entropy(x_reconstructed, x, reduction=\u001b[33m'\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    kl_divergence = -\u001b[34m0.5\u001b[39;49;00m * torch.sum(\u001b[34m1\u001b[39;49;00m + logvar - mu.pow(\u001b[34m2\u001b[39;49;00m) - logvar.exp())\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m reconstruction_loss + kl_divergence\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mone_hot\u001b[39;49;00m(labels, num_classes, device):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.eye(num_classes)[labels].to(device)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\u001b[37m\u001b[39;49;00m\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    kwargs = {\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\u001b[37m\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mIniciando treinamento....\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, **kwargs)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mDataset carregado\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    num_epochs = args.epochs\u001b[37m\u001b[39;49;00m\n",
      "    latent_dim = args.latent_dim\u001b[37m\u001b[39;49;00m\n",
      "    learning_rate = args.lr\u001b[37m\u001b[39;49;00m\n",
      "    num_classes = args.num_classes\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model = CVAE(input_channels=\u001b[34m1\u001b[39;49;00m, latent_dim=latent_dim, num_classes=num_classes).to(device)\u001b[37m\u001b[39;49;00m\n",
      "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mModelo Carregado\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model.train()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m tqdm.tqdm(\u001b[36mrange\u001b[39;49;00m(num_epochs)):\u001b[37m\u001b[39;49;00m\n",
      "        train_loss = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, labels) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\u001b[37m\u001b[39;49;00m\n",
      "            data = data.to(device)\u001b[37m\u001b[39;49;00m\n",
      "            labels = one_hot(labels, num_classes, device)\u001b[37m\u001b[39;49;00m\n",
      "            optimizer.zero_grad()\u001b[37m\u001b[39;49;00m\n",
      "            x_reconstructed, mu, logvar = model(data, labels)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# Verifique as dimensões dos tensores\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34massert\u001b[39;49;00m x_reconstructed.shape == data.shape, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mShape mismatch: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mx_reconstructed.shape\u001b[33m}\u001b[39;49;00m\u001b[33m vs \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdata.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            loss = loss_function(x_reconstructed, data, mu, logvar)\u001b[37m\u001b[39;49;00m\n",
      "            loss.backward()\u001b[37m\u001b[39;49;00m\n",
      "            train_loss += loss.item()\u001b[37m\u001b[39;49;00m\n",
      "            optimizer.step()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch+\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnum_epochs\u001b[33m}\u001b[39;49;00m\u001b[33m], Loss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_loss/\u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\u001b[33m:\u001b[39;49;00m\u001b[33m.4f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_dir = args.model_dir\u001b[37m\u001b[39;49;00m\n",
      "    save_model(model, model_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    torch.save(model.cpu().state_dict(), path)\u001b[37m\u001b[39;49;00m\n",
      "    torch.save(model.decoder.state_dict(), os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mdecoder.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, **kwargs):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    dataset = datasets.MNIST(\u001b[37m\u001b[39;49;00m\n",
      "        training_dir,\u001b[37m\u001b[39;49;00m\n",
      "        train=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        transform=transforms.ToTensor()\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\u001b[37m\u001b[39;49;00m\n",
      "        dataset,\u001b[37m\u001b[39;49;00m\n",
      "        batch_size=batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        shuffle=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        sampler=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        **kwargs\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, test_dir, **kwargs):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\u001b[37m\u001b[39;49;00m\n",
      "        datasets.MNIST(\u001b[37m\u001b[39;49;00m\n",
      "            test_dir,\u001b[37m\u001b[39;49;00m\n",
      "            train=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            transform=transforms.ToTensor(),\u001b[37m\u001b[39;49;00m\n",
      "        ),\u001b[37m\u001b[39;49;00m\n",
      "        batch_size=test_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        shuffle=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        **kwargs\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Carrega o modelo a partir do diretório especificado.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mIniciando a função model_fn para carregar o modelo.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Obter parâmetros de configuração de variáveis de ambiente\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    latent_dim = \u001b[36mint\u001b[39;49;00m(os.getenv(\u001b[33m'\u001b[39;49;00m\u001b[33mLATENT_DIM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m120\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    num_classes = \u001b[36mint\u001b[39;49;00m(os.getenv(\u001b[33m'\u001b[39;49;00m\u001b[33mNUM_CLASSES\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    input_channels = \u001b[36mint\u001b[39;49;00m(os.getenv(\u001b[33m'\u001b[39;49;00m\u001b[33mINPUT_CHANNELS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mParâmetros do modelo: Latent Dim = \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m, Num Classes = \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m, Input Channels = \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                latent_dim, num_classes, input_channels)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Inicializar o modelo\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model = CVAE(input_channels=input_channels, latent_dim=latent_dim, num_classes=num_classes)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mModelo CVAE inicializado.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Carregar o estado do modelo\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mCarregando o modelo do caminho: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, model_path)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        model.load_state_dict(torch.load(model_path, weights_only=\u001b[34mTrue\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mModelo carregado com sucesso.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\u001b[37m\u001b[39;49;00m\n",
      "        logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mErro ao carregar o modelo: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, e)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mraise\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model.eval()\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mModelo definido para modo de avaliação.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, request_content_type):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"An input_fn that loads JSON data with labels as a number from 0 to 10.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mRecebendo dados com o tipo de conteúdo: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, request_content_type)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m request_content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Carregar o JSON do corpo da requisição\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(request_body, \u001b[36mstr\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            data = json.loads(request_body)  \u001b[37m# Directly load JSON if it's a string\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            data = json.loads(request_body.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDados recebidos: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, data)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Garantir que 'label' está presente e é um número entre 0 e 10\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m data:\u001b[37m\u001b[39;49;00m\n",
      "            logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mA chave \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m não está presente no JSON.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mJSON must contain a \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m key with an integer value between 0 and 10.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \u001b[36mint\u001b[39;49;00m) \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m (\u001b[34m0\u001b[39;49;00m <= data[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] <= \u001b[34m10\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mO valor da chave \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m deve ser um inteiro entre 0 e 10: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, data[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mJSON must contain a \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m key with an integer value between 0 and 10.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Converter o número em um tensor\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        label = torch.tensor([data[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]], dtype=torch.int)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mLabel convertido em tensor: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, label)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m label\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melif\u001b[39;49;00m request_content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcessando dados de entrada em formato .npy.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m io.BytesIO(request_body) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "            np_data = np.load(f)  \u001b[37m# Load from the BytesIO object\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        label = torch.tensor(np_data, dtype=torch.int)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mLabel convertido de .npy para tensor: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, label)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m label\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mTipo de conteúdo não suportado: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, request_content_type)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported content type: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mrequest_content_type\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mIniciando a função de predição com os dados de entrada: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, input_data)\u001b[37m\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    latent_dim = \u001b[36mint\u001b[39;49;00m(os.getenv(\u001b[33m'\u001b[39;49;00m\u001b[33mLATENT_DIM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m120\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    num_classes = \u001b[36mint\u001b[39;49;00m(os.getenv(\u001b[33m'\u001b[39;49;00m\u001b[33mNUM_CLASSES\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDimensão latente: \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m, Número de classes: \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, latent_dim, num_classes)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\u001b[37m\u001b[39;49;00m\n",
      "        z = torch.randn(\u001b[34m1\u001b[39;49;00m, latent_dim)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mVetores aleatórios gerados para a predição: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, z.shape)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        labels_one_hot = one_hot(input_data, num_classes, device)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mLabels one-hot gerados: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, labels_one_hot.shape)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        output = model.decoder(z, labels_one_hot)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaída do modelo: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, output)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m output\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# load_dotenv()\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--latent_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m120\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mLatent dim for CVAE (default: 120)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m1000\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m10\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--num-classes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m10\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of classes (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    train(parser.parse_args())\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# Verificando o script que será utilizado\n",
    "!pygmentize training_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"training_script.py\",\n",
    "    role=role,\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.3.0\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\", # coloque um modelo gratuito\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-09-09-16-37-09-689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 16:37:10 Starting - Starting the training job...\n",
      "2024-09-09 16:37:39 Starting - Preparing the instances for training...\n",
      "2024-09-09 16:38:03 Downloading - Downloading input data...\n",
      "2024-09-09 16:38:38 Downloading - Downloading the training image.........\n",
      "2024-09-09 16:39:54 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:09,360 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:09,360 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:09,361 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:09,370 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:09,372 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,624 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,625 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,701 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,702 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,716 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,717 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,727 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m4.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2024-09-09-16-37-09-689\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-testets/pytorch-training-2024-09-09-16-37-09-689/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"training_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"training_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=training_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m4.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=training_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-testets/pytorch-training-2024-09-09-16-37-09-689/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m4.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"pytorch-training-2024-09-09-16-37-09-689\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-testets/pytorch-training-2024-09-09-16-37-09-689/source/sourcedir.tar.gz\",\"module_name\":\"training_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"training_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python training_script.py\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,727 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-09-09 16:40:10,727 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mIniciando treinamento....\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mDataset carregado\u001b[0m\n",
      "\u001b[34mModelo Carregado\u001b[0m\n",
      "\u001b[34m0%|          | 0/10 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch [1/10], Loss: 185.4287\u001b[0m\n",
      "\u001b[34m10%|█         | 1/10 [00:43<06:28, 43.20s/it]\u001b[0m\n",
      "\u001b[34mEpoch [2/10], Loss: 176.2728\u001b[0m\n",
      "\u001b[34m20%|██        | 2/10 [01:29<06:00, 45.08s/it]\u001b[0m\n",
      "\u001b[34mEpoch [3/10], Loss: 176.1603\u001b[0m\n",
      "\u001b[34m30%|███       | 3/10 [02:16<05:19, 45.70s/it]\u001b[0m\n",
      "\u001b[34mEpoch [4/10], Loss: 176.0717\u001b[0m\n",
      "\u001b[34m40%|████      | 4/10 [03:03<04:38, 46.38s/it]\u001b[0m\n",
      "\u001b[34mEpoch [5/10], Loss: 176.0597\u001b[0m\n",
      "\u001b[34m50%|█████     | 5/10 [03:51<03:55, 47.07s/it]\u001b[0m\n",
      "\u001b[34mEpoch [6/10], Loss: 176.1038\u001b[0m\n",
      "\u001b[34m60%|██████    | 6/10 [04:38<03:07, 46.86s/it]\u001b[0m\n",
      "\u001b[34mEpoch [7/10], Loss: 176.1430\u001b[0m\n",
      "\u001b[34m70%|███████   | 7/10 [05:27<02:22, 47.53s/it]\u001b[0m\n",
      "\u001b[34mEpoch [8/10], Loss: 176.0108\u001b[0m\n",
      "\u001b[34m80%|████████  | 8/10 [06:17<01:37, 48.53s/it]\u001b[0m\n",
      "\u001b[34mEpoch [9/10], Loss: 175.9490\u001b[0m\n",
      "\u001b[34m90%|█████████ | 9/10 [07:07<00:49, 49.02s/it]\u001b[0m\n",
      "\n",
      "2024-09-09 16:48:16 Uploading - Uploading generated training model\u001b[34mEpoch [10/10], Loss: 175.9491\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [07:59<00:00, 49.71s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [07:59<00:00, 47.91s/it]\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m2024-09-09 16:48:12,230 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-09-09 16:48:12,230 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-09-09 16:48:12,231 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-09-09 16:48:29 Completed - Training job completed\n",
      "Training seconds: 627\n",
      "Billable seconds: 627\n"
     ]
    }
   ],
   "source": [
    "# Isso pode levar um tempo\n",
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-testets/pytorch-training-2024-09-09-15-55-30-277/output/model.tar.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Crie um objeto Model\n",
    "model = PyTorchModel(\n",
    "    entry_point=\"training_script.py\",\n",
    "    model_data=estimator.model_data,  # Caminho S3 para o modelo treinado\n",
    "    # image_uri=estimator.image_uri,    # Imagem do contêiner usada para o treinamento\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    framework_version='2.3.0', py_version='py311',\n",
    "    env={\n",
    "        \"LATENT_DIM\": \"120\",\n",
    "        \"NUM_CLASSES\": \"10\",\n",
    "        \"INPUT_CHANNELS\": \"1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Implemente o modelo em um endpoint\n",
    "predictor = model.deploy(endpoint_name=\"test-endpoint-13\", initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando o endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chamando o endpoint, você pode fazer pelo teste direto do console se desejar\n",
    "response = predictor.predict([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plote a imagem resultado\n",
    "image = response.squeeze()  # Now it should be of shape (28, 28)\n",
    "\n",
    "# Plotting the image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.title('Model Output Image')  # Title for the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpando o endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
